import re
import string
from typing import Any, Callable, Dict, List, Union

import numpy as np

from haystack import Pipeline
from haystack.core.component import Component
from haystack.evaluation.eval_utils import get_answers_from_output
from haystack.evaluation.metrics import Metric, MetricsResult


class EvaluationResult:
    """
    EvaluationResult keeps track of all the information related to evaluation, namely the runnable (Pipeline or
    component), inputs, outputs, and expected outputs.
    The EvaluationResult keeps track of all the information stored by eval.

    :param runnable: The runnable (Pipeline or component) used for evaluation.
    :param inputs: List of inputs used for evaluation.
    :param outputs: List of outputs generated by the runnable.
    :param expected_outputs: List of expected outputs used for evaluation.
    """

    def __init__(
        self,
        runnable: Union[Pipeline, Component],
        inputs: List[Dict[str, Any]],
        outputs: List[Dict[str, Any]],
        expected_outputs: List[Dict[str, Any]],
    ) -> None:
        self.runnable = runnable
        self.inputs = inputs
        self.outputs = outputs
        self.expected_outputs = expected_outputs

        # Determine the type of the runnable
        if str(type(runnable).__name__) == "Pipeline":
            self.runnable_type = "pipeline"
        else:
            self.runnable_type = "component"

    # pylint: disable=too-many-return-statements
    def calculate_metrics(self, metric: Union[Metric, Callable[..., MetricsResult]], **kwargs) -> MetricsResult:
        """
        Calculate evaluation metrics based on the provided Metric or using the custom metric function.

        :param metric: The Metric indicating the type of metric to calculate or custom function to compute.
        :return: MetricsResult containing the calculated metric.
        """
        if metric == Metric.RECALL:
            return self._calculate_recall(**kwargs)

        elif metric == Metric.F1:
            return self._calculate_f1(**kwargs)

        elif metric == Metric.MRR:
            return self._calculate_mrr(**kwargs)

        elif metric == Metric.MAP:
            return self._calculate_map(**kwargs)

        elif metric == Metric.EM:
            predictions = get_answers_from_output(self.outputs, self.runnable_type)
            labels = get_answers_from_output(self.expected_outputs, self.runnable_type)
            return self._calculate_em(predictions=predictions, labels=labels, **kwargs)

        elif metric == Metric.SAS:
            return self._calculate_sas(**kwargs)

        return metric(self, **kwargs)

    def _calculate_recall(self):
        return MetricsResult({"recall": None})

    def _calculate_map(self):
        return MetricsResult({"mean_average_precision": None})

    def _calculate_mrr(self):
        return MetricsResult({"mean_reciprocal_rank": None})

    def _calculate_f1(self):
        return MetricsResult({"f1": None})

    def _calculate_em(
        self,
        predictions,
        labels,
        regexes_to_ignore=None,
        ignore_case=False,
        ignore_punctuation=False,
        ignore_numbers=False,
    ):
        if len(predictions) != len(labels):
            raise ValueError("The number of predictions and labels must be the same.")
        if len(predictions) == len(labels) == 0:
            # Return Exact Match as 0 for no inputs
            return MetricsResult({"exact_match": 0.0})

        if regexes_to_ignore is not None:
            for s in regexes_to_ignore:
                predictions = np.array([re.sub(s, "", x) for x in predictions])
                labels = np.array([re.sub(s, "", x) for x in labels])
        else:
            predictions = np.asarray(predictions)
            labels = np.asarray(labels)

        if ignore_case:
            predictions = np.char.lower(predictions)
            labels = np.char.lower(labels)

        if ignore_punctuation:
            repl_table = string.punctuation.maketrans("", "", string.punctuation)
            predictions = np.char.translate(predictions, table=repl_table)
            labels = np.char.translate(labels, table=repl_table)

        if ignore_numbers:
            repl_table = string.digits.maketrans("", "", string.digits)
            predictions = np.char.translate(predictions, table=repl_table)
            labels = np.char.translate(labels, table=repl_table)

        score_list = predictions == labels
        em = np.mean(score_list)
        return MetricsResult({"exact_match": em})

    def _calculate_sas(self):
        val = 0
        return MetricsResult({"exact_match": val})


def eval(
    runnable: Union[Pipeline, Component], inputs: List[Dict[str, Any]], expected_outputs: List[Dict[str, Any]]
) -> EvaluationResult:
    """
    Evaluates the provided Pipeline or component based on the given inputs and expected outputs.

    This function facilitates the evaluation of a given runnable (either a Pipeline or a component) using the provided
    inputs and corresponding expected outputs.

    :param runnable: The runnable (Pipeline or component) used for evaluation.
    :param inputs: List of inputs used for evaluation.
    :param expected_outputs: List of expected outputs used for evaluation.

    :return: An instance of EvaluationResult containing information about the evaluation, including the runnable,
    inputs, outputs, and expected outputs.
    """

    outputs = []

    # Check that expected outputs has the correct shape
    if len(inputs) != len(expected_outputs):
        raise ValueError(
            f"The number of inputs ({len(inputs)}) does not match the number of expected outputs "
            f"({len(expected_outputs)}). Please ensure that each input has a corresponding expected output."
        )

    for input_ in inputs:
        output = runnable.run(input_)
        outputs.append(output)

    return EvaluationResult(runnable, inputs, outputs, expected_outputs)
